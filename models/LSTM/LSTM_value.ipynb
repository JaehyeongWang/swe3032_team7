{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"p1Bo6jHYN5Md"},"outputs":[],"source":["#call library\n","\n","import os\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Dense, GRU, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4norTBJZiWC"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","'''\n","이런식으로 사용하면 됩니다.\n","from preprocesslib import preprocessEach, makeY\n","X = preprocessEach('overlap')\n","X bootstrap해서 -> X_final만든 후에\n","y = makeY('overlap', len(X_final))\n","'''\n","\n","def preprocessEach(folder_name):\n","   # set file path\n","    folder_path = base_path + f'data/{folder_name}_clean'\n","\n","    # load file name in folder\n","    file_names = os.listdir(folder_path)\n","    data_array = []\n","\n","    for file in file_names:\n","        # set file path\n","        file_path = os.path.join(folder_path, file)\n","        \n","        # load csv file as dataframe\n","        df = pd.read_csv(file_path)\n","\n","        #preprocessing column\n","        df['open-close'] = df['Open'] - df['Close'] #추가\n","        df['open-close'] = df['open-close'] + abs(min(df['open-close'])) #추가\n","        df['MA5'] = df['Close'].rolling(5).mean() #추가\n","        df['Diff'] = df['High'] - df['Low']\n","        df = df[['Close','Volume','Diff','open-close','MA5', 'Search']]\n","        # df = df[['Close','Volume','Diff','open-close', 'Search']]\n","        df.fillna(0, inplace=True)\n","\n","        #log transform\n","        df = np.log1p(df)\n","\n","        #insert list to array\n","        data_array.append(df)\n","\n","    return np.array(data_array)\n","\n","def makeYValue(folder_name, filelength):\n","    if folder_name=='growth':\n","        yValues = np.zeros(filelength)\n","    elif folder_name=='value':\n","        yValues = np.ones(filelength)\n","\n","    y = np.column_stack([yValues.T])\n","    return y\n","\n","def bootstrap(large, small):\n","    diff = len(large) - len(small)\n","    bootstrapped = small[np.random.choice(len(small), size=diff, replace=True)]\n","    return np.concatenate((small, bootstrapped), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SEEAjHRLN5Mf"},"outputs":[],"source":["xGrowths = preprocessEach('growth')\n","xValues = preprocessEach('value')\n","xOverlaps = preprocessEach('overlap')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvFiG5NwN5Mh"},"outputs":[],"source":["xValues = np.concatenate((xValues, xOverlaps), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZrTtzUHtdFO"},"outputs":[],"source":["xGrowths = bootstrap(xValues, xGrowths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Twi0vvTaN5Mi"},"outputs":[],"source":["#make y values\n","yGrowths = makeYValue('growth', len(xGrowths))\n","yValues = makeYValue('value', len(xValues))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-c7H1b8N5Mj"},"outputs":[],"source":["x = np.concatenate((xGrowths, xValues, xOverlaps), axis=0)\n","y = np.concatenate((yGrowths, yValues, yOverlaps), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9Ly7XHjN5Mk"},"outputs":[],"source":["#minmax scaling by element\n","scaler = MinMaxScaler()\n","for i in range(6):\n","    x[:,:,i] = scaler.fit_transform(x[:,:,i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YKrIXSuCN5Ml"},"outputs":[],"source":["def splitData(x, y, train_ratio=0.8):\n","    # shuffle data\n","    np.random.seed(2023)\n","    permutation = np.random.permutation(len(x))\n","    x = x[permutation]\n","    y = y[permutation]\n","\n","    # split data\n","    n_train_samples = int(train_ratio * len(x))\n","    x_train, x_test = x[:n_train_samples], x[n_train_samples:]\n","    y_train, y_test = y[:n_train_samples], y[n_train_samples:]\n","\n","    return x_train, x_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Um2iM4duH23"},"outputs":[],"source":["X_train, X_test, Y_train, Y_test = splitData(x,y,0.8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vj8GiSwuJ6k"},"outputs":[],"source":["X_train = np.nan_to_num(X_train, nan=0)\n","X_test = np.nan_to_num(X_test, nan=0)\n","\n","Y_train = to_categorical(Y_train)\n","Y_test = to_categorical(Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsWtsdCoN5Ms"},"outputs":[],"source":["es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=300)\n","mc = ModelCheckpoint('best_model_value.h5', monitor='val_acc', mode='max', save_best_only=True)\n","\n","def create_model():\n","    model = Sequential()\n","    model.add(GRU(64, input_shape=(1248,6), return_sequences=True))\n","    model.add(Dropout(0.2))\n","    model.add(GRU(16))\n","    model.add(Dense(2, activation='softmax'))\n","    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=300)\n","    mc = ModelCheckpoint('best_model_value.h5', monitor='val_acc', mode='max', save_best_only=True)\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5rIjBzoacqv"},"outputs":[],"source":["#split data for cross validation\n","def splitDataCrossVal(x, y, fold=3):\n","    # shuffle data\n","    permutation = np.random.permutation(len(x))\n","    x = x[permutation]\n","    y = y[permutation]\n","\n","    x_split = []\n","    y_split = []\n","    # split data\n","    n_samples = int(len(x)/fold)\n","    for i in range(fold-1):\n","      x_split.append(x[i*n_samples:(i+1)*n_samples])\n","      y_split.append(y[i*n_samples:(i+1)*n_samples])\n","    x_split.append(x[(fold-1)*n_samples:])\n","    y_split.append(y[(fold-1)*n_samples:])\n","\n","    return x_split, y_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6BfL3-c3qeq"},"outputs":[],"source":["def cvsplitData(x,y, ind):\n","  x_test = x[ind]\n","  y_test = y[ind]\n","  cnt = 0\n","  for i in range(len(x)):\n","    if i!=ind:\n","      if cnt == 0:\n","        x_train = x[i]\n","        y_train = y[i]\n","        cnt += 1\n","      else:\n","        x_train = np.concatenate((x_train, x[i]))\n","        y_train = np.concatenate((y_train, y[i]))\n","  return x_train, x_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2Hbslc_uQql"},"outputs":[],"source":["x_split, y_split = splitDataCrossVal(X_train, Y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfWe-kRjarpv"},"outputs":[],"source":["print(len(x_split))\n","accuracy_cross_val = []\n","for i in range(len(x_split)):\n","  x_train, x_test, y_train, y_test = cvsplitData(x_split, y_split, i)\n","  x_train = pad_sequences(x_train, dtype='float32')\n","  x_test = pad_sequences(x_test, dtype='float32')\n","  x_train = np.nan_to_num(x_train, nan=0)\n","  x_test = np.nan_to_num(x_test, nan=0)\n","\n","  model = create_model()\n","\n","  history=model.fit(x_train, y_train, epochs=2000, validation_data=(x_test, y_test), callbacks=[es, mc])\n","  model=load_model('best_model.h5')\n","  loss, accuracy = model.evaluate(x_test, y_test)\n","  accuracy_cross_val.append(accuracy)\n","  with open(f'history_value{i}.json', 'w') as f:\n","      json.dump(history.history, f)\n","print(\"cross validation accuracy:{}\".format(accuracy_cross_val))\n","print(\"average cross validation accuracy:{}\".format(sum(accuracy_cross_val)/len(accuracy_cross_val)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qc79y32Aa_dd"},"outputs":[],"source":["epochs = range(1, len(history.history['acc']) + 1)\n","plt.plot(epochs, history.history['loss'])\n","plt.plot(epochs, history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxdgNix_ov-I"},"outputs":[],"source":["epochs = range(1, len(history.history['acc']) + 1)\n","plt.plot(epochs, history.history['acc'])\n","plt.plot(epochs, history.history['val_acc'])\n","plt.title('model acc')\n","plt.ylabel('acc')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
